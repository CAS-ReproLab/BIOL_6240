{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28e30bfd-961b-44dc-8eec-bef3ad27f89a",
   "metadata": {},
   "source": [
    "# Homework 4 Data Science Crash Course for Biochemistry Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b8065-6d0d-4da1-96dd-12507873ca2c",
   "metadata": {},
   "source": [
    "## Before you begin:\n",
    "\n",
    "You'll need the following libraries installed in your Python environment: Pandas, Numpy, Matplotlib, and Seaborn.\n",
    "\n",
    "1. You don't need to import libraries as often as I do here. I usually just import them all in one code cell at the very top of the notebook. However in this tutorial I wanted each cell to be self contained.\n",
    "2. If you're running this in google colab, you won't be able to write files and read files directly without mounting them into first. [Here's some directions on how to do that](https://www.geeksforgeeks.org/ways-to-import-csv-files-in-google-colab/). \n",
    "3. You could do all this in a developer environment rather than jupyter notebooks. The process of running and displaying the code is a little different. I really like the spyder IDE if you are looking for a good option to try out. The [documentation page](https://docs.spyder-ide.org/current/videos/first-steps-with-spyder.html) has videos on how to get started. That said, I think jupyter notebooks are actually pretty great for doing data science since they have integrated markdown cells for you to annotate the data as you go. You can download spyder through the Anaconda navigator if you're so inclined.\n",
    "\n",
    "Most of the code blocks below just need to be run to see (and learn from) the output. The places where you are asked to writed code are indicated in the question cell or directly in the code comments. Don't be afraid to modify things and test them out. You can copy the cells before making changes if desired. If you break something, just download the original again.\n",
    "\n",
    "To learn more about some of the libraries used in this tutorial: \n",
    "\n",
    "* [numpy](https://numpy.org/devdocs/user/quickstart.html)\n",
    "* [pandas](https://pandas.pydata.org/docs/user_guide/10min.html)\n",
    "* [matplotlib.pyplot](https://matplotlib.org/stable/tutorials/pyplot.html)\n",
    "* [seaborn](https://seaborn.pydata.org/tutorial.html)\n",
    "\n",
    "The .csv file required for this exercise is in the [Github](https://github.com/CAS-ReproLab/BIOL_6240/tree/main/Homework) (or on Canvas for BIOL 6240 students), it contains ~7 thousand entries for different chemical compounds. (*PubChem Compound TOC: Drugs). Additional datasets are available at [pubchem datasource](https://pubchem.ncbi.nlm.nih.gov/classification/#hid=72). Once you complete this tutorial, I encourage you to find a dataset you're interested in and continue your skill development by engaging in data analysis that interests you. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24029e51-aef8-45d5-abef-c4d4fe3f88f6",
   "metadata": {},
   "source": [
    "## Part I Little Data\n",
    "\n",
    "Computer programs provide instructions for a computer to execute. Overall these instructions consist of only a few basic ingredients: \n",
    "\n",
    "1. Information stored as variables- these can take different forms depending on the structure of the data that needs stored. We'll call these different forms Data Structures.\n",
    "2. Functions- these are modular subsets of instructions within the program. They can be looped through many times if needed.\n",
    "3. Control flow- the process of looping through subsets of instructions is called control flow.\n",
    "\n",
    "That's really it. There are a few other spefic things that can be helpful for making bigger programs more functional, but ultimately getting the hang of just these three ingredients will set you up to program your way through any project you undertake.\n",
    "\n",
    "In this part, let's focus on a few useful Python specific data structures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9886b36-1626-4d3a-8169-2060f2a4a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "my_string = 'string' #strings are characters and are defined by '' or \"\" \n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fc3c6d-2a20-4891-91de-4f02e6bb320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists\n",
    "my_list = [1, 2, 3, 'a', 'b', 'c'] # lists store sets of variables of different types. Here we have a list of numbers and string letters\n",
    "print(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfdac90-f823-483e-9895-985ff6d1ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigating Lists\n",
    "print(my_list[0]) #using list_name[index] lets us access specic list elements. The list index starts at 0 (the first item) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa038d74-630c-4f58-9641-9753af70c2f1",
   "metadata": {},
   "source": [
    "### Question 1: Write code to access the last element in the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6221f418-b481-42f0-a87e-a3c9811d88a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba6844a-3409-47e0-9239-ca67f50d949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists can also be navigated backward\n",
    "print(my_list[-1]) # prints the last element. It's like taking a step backward from the first (0) position in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83be06d7-7bf8-4b2f-a3b5-9c38d5448a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries\n",
    "my_dict={\"Col1\":[1,2,3], \"Col2\": ['a','b','c']} # dictionaries hold 'keys' with associated lists of values. Think of these as columns and rows.\n",
    "print(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5728fb-f036-476d-ac86-c045a52db8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries are useful ways to store data, but they aren't visually appealing to look at. \n",
    "# Let's import the pandas data science library so we can use a better data structure for this. It's called a dataframe\n",
    "import pandas\n",
    "\n",
    "my_dict={\"Col1\":[1,2,3], \"Col2\": ['a','b','c']} # Here's our dictionary again\n",
    "print(my_dict)\n",
    "\n",
    "my_dataframe = pandas.DataFrame(my_dict) # Here's how we convert the dictionary into a dataframe. Dataframes are AWESOME for tabular data that have columnas and rows.\n",
    "print(my_dataframe)\n",
    "\n",
    "my_dataframe.head() # Pandas also has a bunch of useful tools that we can invoke like this .head() method that displays only the first five lines of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a55205-7d18-4fde-8474-d57372798109",
   "metadata": {},
   "source": [
    "### Question 2: Write some code to add a third column to my_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5c8c7-adfe-42c2-861e-8ba35edb368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a7f1c4-fb41-4e1b-8a47-2f0e0edc73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with Data\n",
    "AminoAcids=['alanine', 'proline', 'glutamine'] # list of amino acids as strings\n",
    "Atomic_Mass= [71.08, 97.12, 128.13] # list of atomic masses for each ammino acid as float values (decimals)\n",
    "\n",
    "# Now let's make a dataframe\n",
    "import pandas as pd # we can change the name we import as to make it easier to use the tools in the library. I.e., pandas.dataframe() Vs. pd.dataframe(). These do the same thing.\n",
    "\n",
    "AA_df=pd.DataFrame({'Amino Acids': AminoAcids, 'Atomic Mass': Atomic_Mass}) # {Column Header': list, ...}\n",
    "\n",
    "AA_df.head() # Show the first five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b11e9-9119-443c-9911-8b6d45e67760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can interact with the dataframe in a number of ways\n",
    "print(AA_df['Amino Acids']) # Prints only the first column\n",
    "\n",
    "print(AA_df['Atomic Mass'] < 100) # Prints the 'boolean' truth value of the statement: the atomic mass is < 100 for each amino acid\n",
    "\n",
    "# We can use pandas to add the truth values to the dataframe\n",
    "AA_df['Cutoff']=AA_df['Atomic Mass'] < 100\n",
    "print(AA_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a25c0b-bf45-4276-9f31-57983974bf12",
   "metadata": {},
   "source": [
    "### Question 3: Add a column to the AA_df dataframe that multiples the Atomic Mass by 2. \n",
    "\n",
    "Note: for python the * symbol indicates multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176c2ca-aaa7-4520-9911-4ff1f2929c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AA_df['Amino Acids']) # Prints only the first column\n",
    "\n",
    "print(AA_df['Atomic Mass'] < 100) # Prints the 'boolean' truth value of the statement: the atomic mass is < 100 for each amino acid\n",
    "\n",
    "# We can use pandas to add the truth values to the dataframe\n",
    "AA_df['Cutoff']=AA_df['Atomic Mass'] < 100 # New column = calculation on column in the current dataframe\n",
    "print(AA_df)\n",
    "\n",
    "# Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff80a61-bb96-4023-8a3b-3a6702a0d0a8",
   "metadata": {},
   "source": [
    "## Part II Big Data\n",
    "\n",
    "It might not be clear why we wouldn't just do this in Microsoft Excel. The code above is onerous for small dataframes. Where it really shines is when dealing with big datasets. \n",
    "\n",
    "In this part we'll learn how to use tools from the numpy (pronounced NUM-PIE) library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f8cf3f-4acd-4fba-bf7e-92dcc0757345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataframes we just used are actually built on special structures called numpy arrays\n",
    "# numpy arrays are like lists, but different in some important ways\n",
    "import numpy as np\n",
    "\n",
    "my_list = [1,2,3]\n",
    "print(my_list)\n",
    "\n",
    "my_array = np.array([1,2,3])\n",
    "print(my_array)\n",
    "\n",
    "# They might look similar, but they're quit different, let's try something: \n",
    "new_list = 2 * my_list\n",
    "print(new_list)\n",
    "\n",
    "new_array = 2 * my_array\n",
    "print(new_array)\n",
    "\n",
    "# See the difference:) Numpy arrays are basically matrices, which you might recall from your linear algebra nightmares.. All jokes aside, they're SUPER useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c54069-50e9-4329-b1d2-1dc93364fa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the arange function from numpy to make a big array..\n",
    "big_array = np.arange(0, 100, 10) # makes an array from 0-100 in steps of 10\n",
    "print(big_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65aadca-b9da-4f62-aa27-6d050217e861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, it's not that big of an array. \n",
    "\n",
    "BIG_array = np.arange(1,1000, 0.1) # from 1-1000 in steps of 0.1\n",
    "print(BIG_array)\n",
    "\n",
    "print(\"The length of the BIG_array is:\", len(BIG_array), \"points\") # prints the length of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb80f55a-0c23-4aa9-b179-65c6afc70fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's use these numpy tools and the methods we talked about earlier to make a BIG dataframe...\n",
    "import numpy as np\n",
    "\n",
    "Column_1 = np.arange(0, 1000, 0.1)\n",
    "Column_2 = np.arange(0, 1000, 0.1) ** 2 # we can square this array using the exponent symbol ** \n",
    "Column_3 = np.arange(0, 1000, 0.1) ** 3 # now we're getting cubical..\n",
    "\n",
    "# Let's make a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "BIG_df=pd.DataFrame({\"Original\": Column_1, \"Square\": Column_2, \"Cubed\": Column_3})\n",
    "\n",
    "BIG_df.head() # Can also try BIG_df.tail() to see the last five rows. Give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c0dea7-7eab-437a-ba4c-645915e4f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we want to save the dataframe we just made? \n",
    "import pandas as pd\n",
    "\n",
    "BIG_df.to_csv('big.csv', index = False, header = True) # This will save in your current folder. Open it with excel to see its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1426e71-91eb-42ed-900b-aab6be9c80a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we want to open a .csv file? This is more likely since we might want to work with big database files we've downloaded or generated in the lab\n",
    "import pandas as pd\n",
    "imported_df=pd.read_csv('big.csv') # create a dataframe to store it, and read in the dataframe using pd.read_csv()\n",
    "imported_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613af4d2-ac9c-4101-a94b-04a5fc7c49fd",
   "metadata": {},
   "source": [
    "### Question 4: Add a column to the dataframe that is first column raised to the fourth power.\n",
    "\n",
    "Use the method you used in the AA_df exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1b2da0-fb9a-4ddd-a587-4d392eb3758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Answer Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f06804-9ceb-478c-9e86-c72003fdfcb9",
   "metadata": {},
   "source": [
    "## Part III: Data Visualization\n",
    "\n",
    "In this part we'll learn about some basic plotting features available in Python. The library we'll use is called matplotlib. There are others, but this is the most popular one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5806fe68-f16e-4e86-b641-8641be870e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a super simple plot using matplotlib. We can do this in just 5-lines of code!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the data to plot on each axis\n",
    "x_axis = [0, 1, 2, 3, 4, 5]\n",
    "y_axis = [0, 2, 4, 6, 8, 10]\n",
    "\n",
    "plt.plot(x_axis, y_axis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e839725-9065-48f0-ab24-05a611c42318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty bare bones... Let's dress it up a bit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the data to plot on each axis\n",
    "x_axis = [0, 1, 2, 3, 4, 5]\n",
    "y_axis = [0, 2, 4, 6, 8, 10]\n",
    "\n",
    "plt.figure(figsize=(5,3)) # Changes the size of the figure in the display\n",
    "plt.plot(x_axis, y_axis, color='red') # Try replaceing 'plot' with 'scatter' or 'stem'\n",
    "plt.title('Awesome Plot')\n",
    "plt.xlabel('X-Axis')\n",
    "plt.ylabel('Y-Axis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c02e81-22d3-4d25-a69b-fd8c1a9bb438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib has TONS of features. Let's try the style feature.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the data to plot on each axis\n",
    "x_axis = [0, 1, 2, 3, 4, 5]\n",
    "y_axis = [0, 2, 4, 6, 8, 10]\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.style.use('dark_background') # Try replacing 'dark_background' with 'Solarize_Light2' or 'ggplot'\n",
    "plt.plot(x_axis, y_axis, color='red') # Try replaceing 'plot' with 'scatter' or 'stem'. Replace 'red' with 'blue' or 'cyan'\n",
    "plt.title('Awesome Plot')\n",
    "plt.xlabel('X-Axis')\n",
    "plt.ylabel('Y-Axis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf4dc11-b3da-4b1a-9431-bad896f5534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've seen some simple plotting, let's use numpy arrays to generate some data and look at a few different plot types. \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Column_1 = np.arange(0, 1000, 0.1)\n",
    "Column_2 = np.arange(0, 1000, 0.1) ** 2 # we can square this array using the exponent symbol ** \n",
    "Column_3 = np.arange(0, 1000, 0.1) ** 3 # now we're getting cubical..\n",
    "\n",
    "# Let's make a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "BIG_df=pd.DataFrame({\"Original\": Column_1, \"Square\": Column_2, \"Cubed\": Column_3})\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.style.use('dark_background')\n",
    "plt.plot(BIG_df['Original'], BIG_df['Square']) # We reference the dataframe columns as our x and y axes for the plot; Try plotting the 'Cubed' column\n",
    "plt.title('Awesome Plot')\n",
    "plt.xlabel('Original')\n",
    "plt.ylabel('Squared')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0db07c-0fd8-4546-a216-7296a2f8d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to plot a histogram\n",
    "\n",
    "# Use a numpy array to create a collection of values drqwn from a normal (Gaussian) distribution\n",
    "# Parameters\n",
    "mean = 0 \n",
    "stdev = 1\n",
    "sample_size = 1000\n",
    "\n",
    "# Simulate the array\n",
    "normal_dist = np.random.normal(mean, stdev, sample_size)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.style.use('dark_background')\n",
    "plt.hist(normal_dist, bins = 20, alpha=0.75, color='cyan', edgecolor='white') # plot the histogram. Bins are how big the intervales of each bar are.\n",
    "plt.title('Awesome Histogram')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Histograms are a great way to see how the values in a particular column of a dataframe are distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2125c4-9bc6-4c1d-9bb2-2e70fff20ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try this on the dataframe from above\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Column_1 = np.arange(0, 1000, 0.1)\n",
    "Column_2 = np.arange(0, 1000, 0.1) ** 2 # we can square this array using the exponent symbol ** \n",
    "Column_3 = np.arange(0, 1000, 0.1) ** 3 # now we're getting cubical..\n",
    "\n",
    "# Let's make a dataframe\n",
    "\n",
    "BIG_df=pd.DataFrame({\"Original\": Column_1, \"Square\": Column_2, \"Cubed\": Column_3})\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.style.use('dark_background')\n",
    "plt.hist(BIG_df['Square'], bins = 100, alpha=0.75, color='cyan', edgecolor='white') # plot the histogram. Bins are how big the intervales of each bar are.\n",
    "plt.title('Another Awesome Histogram')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5460aa4f-97ed-4160-8b27-3c237e4dd41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also display some simple statistics using pandas built-in methods\n",
    "# These are called pandas attributes. You can google that term to see a full list of attributes you can use\n",
    "print(\"the mean is\", BIG_df['Original'].mean())\n",
    "print(\"the standard deviation is\", BIG_df['Original'].std())\n",
    "# Take a guess at how you can update this code to print the median and give it a try below.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ffdf50-29de-4501-8fd7-6ec4d62acc89",
   "metadata": {},
   "source": [
    "## Part IV: Some Real Data Science\n",
    "\n",
    "Now we've got the skills to play around with some real data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802ffadc-8967-49c4-9744-8c9332a51cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data as a pandas dataframe\n",
    "import pandas as pd\n",
    "drugs_df = pd.read_csv('PubChem_Drugs.csv')\n",
    "\n",
    "drugs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d88d43b-dc20-493c-981c-279adac05ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas also has a few more useful methods that we can use to check the data\n",
    "# dataframe.describe() will display descriptive statistics for each column\n",
    "drugs_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9131e33-7686-44f1-95c7-914a40728e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe.count() shows the column headers (left) and the number of elements in each column (right)\n",
    "drugs_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ebed94-758b-4e6f-ae95-4baf0dd3a589",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making a Histogram plot to see the data distribution for specified columns\n",
    "\n",
    "# import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Identify the column from the dataframe we want to use for the histogram plot\n",
    "data = drugs_df['xlogp'] # designates the partition coefficient (xlogp) column as the data to be plotted\n",
    "\n",
    "# make the plot\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.style.use('dark_background')\n",
    "plt.hist(data, bins = 100, alpha=0.75, color='cyan', edgecolor='white')\n",
    "plt.xlabel('xlogp')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# We can also save this figure as a .png image\n",
    "plt.savefig('xlogp.png', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c864c78-bc9a-4dd7-9407-a62a97dd2a01",
   "metadata": {},
   "source": [
    "### Question #5: Plot Interpretation: \n",
    "\n",
    "XLogP is the partition coefficient of a drug in octanol or water. A higher [partition coefficient](https://en.wikipedia.org/wiki/Partition_coefficient) indicates that a drug is more hydrophobic. I.e. accumulates more in octanol during an organic extraction. \n",
    "\n",
    "How do you interpret the distribution of this plot? What is the approximate average? What proportion of drugs in the dataset are likely to be water soluble (i.e., xlogp < 5). \n",
    "\n",
    "Your answer here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a0e86-a5ed-43f8-b70b-d73797babe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a scatter plot to assess whether there are any correlations among the columns in the dataset\n",
    "# We may want to change the columns we look at with this plot, so we're going to set it up a little differently\n",
    "\n",
    "# import the relevant libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a plotting function:\n",
    "# We're going to write a function to make our plot\n",
    "# This enables us to choose which columns from the dataframe to plot in a flexible way\n",
    "def plot_dataframe(df, x_col, y_col): # This creates the function and indicates what arguments we will provide to it\n",
    "    plt.figure(figsize=(5,3)) # set figure size for display \n",
    "    plt.style.use('dark_background') # set plot style\n",
    "    plt.scatter(df[x_col], df[y_col], color='cyan', s=2) # make the plot with generic x_col and y_col; s is dot size\n",
    "    plt.title(f'plot of {x_col} Vs. {y_col}') # uses f-string to make the title automatically\n",
    "    plt.xlabel(x_col) # plots the x axis label automatically\n",
    "    plt.ylabel(y_col) # same for y axis\n",
    "    \n",
    "    # We can also save this figure as a .png image\n",
    "    # plt.savefig('figure_name.png', dpi=300) # remove the '#' to save the figure if desired\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# This is how we make the plot\n",
    "# insert any column headers in parentheses to plot them against each other\n",
    "plot = plot_dataframe(drugs_df, 'hbondacc', 'mw') # try 'xlogp' vs 'mw'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd82e0-8f5f-4ee5-bd78-52f4905dc218",
   "metadata": {},
   "source": [
    "### Question #6: Pairwise correlations\n",
    "\n",
    "Choose a pair of columns from drugs_df and plot them against each other using the above plotting function. Describe the relationship between them. \n",
    "\n",
    "Your answer here: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b68cf6-c9ac-4b4d-ab74-b251f18fcfc5",
   "metadata": {},
   "source": [
    "## Part V Lipinkski's Rule of 5\n",
    "\n",
    "According to [lipinski's rule of 5](https://en.wikipedia.org/wiki/Lipinski%27s_rule_of_five) poor absorption is more likely to occur when there are more than (i) 5 hydrogen-bond donors, (ii) 10 (5 × 2) hydrogen-bond acceptors, (iii) a molecular weight greater than 500 (5 × 100), and (iv) a calculated partition coefficient (XLogP) greater than 5. This means that drug candidates that do not meet these criteria are more likely to have high oral availability. \n",
    "\n",
    "In our dataset, the column headers: hbondacc = hydrogen-bond acceptor count, mw = molecular weight, hbonddonor = hydrogen-bond donor count, xlogp= the partition coefficient (XLogP).\n",
    "\n",
    "Next up, we'll use some of the skills we've developed to analyze the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15df884-9a09-42b2-9c85-84f9edddf849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's display our dataframe so we can see the column headers\n",
    "drugs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce046ec-49ac-4ae8-9004-7cec48927789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to make a new dataframe with only the relevant columns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make a new dataframe with only the desired columns for assessing Lipinski's rule \n",
    "lipinski_df=drugs_df[['cid', 'cmpdname', 'mw', 'xlogp', 'hbonddonor', 'hbondacc']]\n",
    "\n",
    "lipinski_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe944ba7-f383-41bd-afbd-98d6716c2283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a better pairwise correlation plot using the seaborn library\n",
    "# Note you may need to install seaborn using the anaconda navigator\n",
    "import seaborn as sns # another plotting library\n",
    "\n",
    "# Make a pairwise correlation plot\n",
    "# Note this may take a minute or two\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.pairplot(lipinski_df)\n",
    "# plt.savefig('figure_name.png', dpi=300) # Uncomment to save the figure if desired\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d6e15-ed66-4b02-b794-d4e8fb8b6ae7",
   "metadata": {},
   "source": [
    "### Question #7: Interpret the plot\n",
    "\n",
    "The pairwise correlation plot compares the scatterplots of all of the columns at once. The middle plots are the histograms of each individual column. \n",
    "\n",
    "Which variables appear to have some correlation?\n",
    "\n",
    "Your answer here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5316a-e789-45b5-893c-fd4777036433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also make another type of correlation plot that might be easier to interpret\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make a correlation matrix\n",
    "correlation_matrix=lipinski_df.corr()\n",
    "\n",
    "# make a heatmap correlation plot\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.heatmap(correlation_matrix, vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e2925b-7e2e-4e22-8426-afef4c5bbeff",
   "metadata": {},
   "source": [
    "### Question #8: Interpret the plot\n",
    "\n",
    "The heatmap makes the correlation coefficients easier to see. Which variables appear to have some correlation? Which appear to be positively correlated? Negatively correlated? \n",
    "\n",
    "Note: the correlations with cid won't make any sense since that column is just a set of id #s. \n",
    "\n",
    "Your answer here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e131ca17-6451-4fbf-8ed3-b07554543e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining a set of drugs that meet Lipinski's rule\n",
    "\n",
    "'''\n",
    "According to lipinski's rule of 5: poor absorption is more likely to occur \n",
    "when there are more than (i) 5 hydrogen-bond donors, \n",
    "(ii) 10 (5 × 2) hydrogen-bond acceptors, \n",
    "(iii) a molecular weight greater than 500 (5 × 100), \n",
    "and (iv) a calculated partition coefficient (XLogP) greater than 5. \n",
    "This means that drug candidates that do not meet these criteria are more likely \n",
    "to have high oral availability.\n",
    "'''\n",
    "\n",
    "# In our last data science task, we'll obtain an save a .csv file containing drug candidates that meet the rule of 5\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "\n",
    "# All we need to do here is make a new dataframe from lipinski_df that includes only the rows that meet our cutoffs\n",
    "# First we'll store our cutoffs as conditions\n",
    "# This isn't strictly necessary to perform in a separate step, but it makes the program cleaner and easier to read\n",
    "conditions=(lipinski_df['mw'] < 500) & (lipinski_df['xlogp'] < 5) & (lipinski_df['hbonddonor']  < 5) & (lipinski_df['hbondacc'] < 10) \n",
    "\n",
    "candidates_df=lipinski_df[conditions] # make your new dataframe\n",
    "\n",
    "candidates_df.to_csv('candidates.csv', index = False, header = True) # Save the data as a .csv in the current directory\n",
    "\n",
    "print(candidates_df.head()) # Check the first five lines to make sure it looks good; These drugs should all have high predicted oral bioavailability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e1d6b7-90f9-4677-abb8-66826b72b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As one very last exercise, let's check the difference between the two dataframes to see how much things changed\n",
    "print(lipinski_df.shape) # Returns the numer of rows, columns in the dataframe\n",
    "print(candidates_df.shape)\n",
    "\n",
    "# Looks good!\n",
    "print(f' There were {lipinski_df.shape[0]} drugs in the original dataframe')\n",
    "print(f' After filtering by Lipinskis Rule, there were {candidates_df.shape[0]} drug candidates identified')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdab793-4732-480c-ab00-7cf01823e0ca",
   "metadata": {},
   "source": [
    "# Now you're a Data Scientist! \n",
    "\n",
    "Go forth and index."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
